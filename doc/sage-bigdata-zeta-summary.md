[TOC]
sage-bigdata-etl 设计文档
===================

version: 1.0

# 1. 引言

##1.1 背景
  sage-bigdata-etl 是公司多个项目需求,结合市面上众多的数据采集、收集 大数据计算的的使用中,根据大量的需求，自主研发一款广泛只是数据输入,性能更优异的集数据采集,处理,转换,增强,分析,聚合于一体的软件。

# 2. 总体设计

  根据需求要求，sage-bigdata-etl要能够接受:

- 多种输入数据源; 
- 多种数据输出(存储); 
- 支持各种数据格式的解析;
- 各种数据的转换,补充;
- 单一,多种 数据源的(聚合)计算;
- 能够运行于不同的数据平台；
- 提供统一的展示平台；
- 集中化配置数据采集;
- 保证采集过程中数据传输的安全,对于接入的访问保证有足够的安全机制防止对数据的非法访问与篡改；
- 提供全局的数据访问接口;

##2.1 数据来源


  支持包括但不限于交换机，路由器，防火墙，WINDOWS、LINUX等操作系统，MYSQL、ORACLE、POSTGRESQL、SQL
Server等数据库系统，HTTP（RESTFULL
API）、ftp、HDFS等各种传输协议等，以及相关的数据API。

##2.2 数据解析格式支持(处理)


- 正则
- JSON 
- XML 
- 使用规则化分隔符的文本 
- 支持常用协议的数据格式（如CEF、syslog等）的解析。

## 2.3 数据增强,转换
- 增加字段
- 删除字段
- 过滤数据
- 字段转义
- 重命名等
 ## 2.4 数据输出
- 常用关系型数据库
  - 非关系型数据库(ES等)
  - kafka等流系统
  - HDFS,ftp,sftp,本地 等文件系统
## 2.5 数据流分析聚合
- 基础的流式SQL
- 基础的流式CEP
- 算法插件
## 2.5 存量数据分析聚合
- 基础的流SQL
- 复杂的计算平台

##2.6 数据平台
- Windows
- L(U)INUX、
- MAC OS 
- AIX 
- 其他常用操作系统。

##2.7 采集方式
- 主动采集(sever) 

- 被动采集(agent)
##2.8 管理方式
- 集中化管理 

- 远程控制

- 提供API管理接口

- 提供服务接口

- HA 保证服务没有单点

## 总体结构设计

根据以上要求系统总体设计结构如下图

![sage-bigdata-etl总体设计图](assets/sage-bigdata-etl.struct.png)



3.功能设计
==========

- 支持主、被动数据采集;
- 支持多种数据格式解析;
- 夸平台部署，采集统一管理，数据集中展示;
- 数据web展示;
- 采集管理;
- 多机分布式部署;

![sage-bigdata-etl总体设计图](assets/sage-bigdata-etl-summary.png)

​							sage-bigdata-etl 概要功能图

3.1 配置管理
------------

  系统设计是采用JSON作为进行数据采集配置的存储与转发的格式，并统一使用server来配置。

3.2 采集
--------

- 能够部署在Windows、Linux等多种操作系统上；
- 支持主动拉去、或者被动接收数据功能；
- 支持多种数据源的数据收取与解析、转发，对采集的数据做归一化；
- 支持多种数据输出方式；

3.3 展示
--------

  系统通过对接一致的数据存储（采用ELASTICSEARCH）结合WEB服务器统一展示数据，并提供相应的查询、检索，数据分析，报表展现等功能。

3.4 访问接口
------------

  根据需求，sage-bigdata-etl
需要提供统一的数据访问接口，支持通过应用程序、WEB等访问访问采集到的数据。

3.5 数据安全 
-------------

1\. 应保证在数据采集工程中，数据传输的安全，校验通过接口访问的应用程序等有足够的数据使用权限。

2. 对于采集，当系统某个节点出现问题停止运行后，保证能够实现数据续传，不重复，不丢失数据。

3\. SAAS 新增云端存储要增加用户识别，日志鉴权等相关的授权

3.6 监控
--------

对于配置的每个数据源，监控启动，处理，关闭，错误等采集代理的变化；

1.  对于每一个配置的采集代理，监控启动，运行，关闭，异常等信息的变化；

2. SAAS安全鉴权

   ## 4.接口设计


根据功能设计，DATAVIEWER
满足功能设计，需要保证各个功能模块之间顺利、流畅的执行，并能够适应多种环境要求，支持分布式，夸网络等特殊情况。

4.1 采集
--------

启动：采集在不同平台下，要有相似的启动功能，发现数据源配置，及相关输出配置，根据数据源配置识别数据源。

运行：

1.  对于配置管理分发的工作任务，自动鉴别出自己的采集工作；

2.  对于制动的工作任务自动识别数据源，并检查数据源是否可用，报告给配置管理；

3.  对于数据源根据配置的读取方法进行读取，检查是否能正常读取，并报告；

4.  对读取的数据进行解析、归一化，检查解析规则是否正确，并报告；

5.  持续运行，识别有没有新任务到来，是否停止正在执行的任务，重新执行任务。

停止：

1.  有统一的停止功能，可以通过管理端（或者手动）停止；

2.  在正常停止时，应能够对于正在运行的任务，顺序安全的停止；

3.  对于每一个任务停止时，要保证采集的数据已经完全入库，不丢失；

4.  同时，对于一些任务记入采集到的数据位置、时间戳等，保证在下次启动时能续读，保证不漏读，重读；

5.  对于一些任务要实时监控数据的变化，及时采集新到来的数据，保证实时性要求；

6.  对于采集到的数据，保证解析正确的录入到合适的数据库位置，对于解析出错的信息要保留原始接入，存储到合适的位置，以保证数据的完整性，不丢、不漏、不重数据；

在采集过程中，当数据存储出现问题时要保证数据不丢弃，在数据库可用时，能够续存（有缓存，或者等待机制）。

采集处理流程图

4.2 配置管理
------------

接收输入

1.  指定的采集器（位置，标示符）；

2.  指定采集数据源位置（文件目录，网络文件地址，传输协议IP+端口等）；

3.  指定数据源读取方式（文件按行，数据库按列，使用的传输协议等）；

4.  指定的数据源解析方式，既每一条读取数据怎么处理（JSON、XML、分割符等）；

> 组织成CONFIG,并保存.
>
> 输出：

1.  通过WEB方式将配置信息发送到RESTful API Server.

2.  生成配置信息，配置状态，Metric信息，还有运行状态.

3.  配置文件信息.

4.3 监控
--------

1.  检查采集器是否已经启动；

2.  对于分配给采集器的采集任务，能否识别，顺利执行；

3.  启动或者重启、关闭采集器；

4.  对于配置错误的采集任务，重新配置，分发任务；

5.  监控任务执行状态。

6.  传递Metric信息给RESTful API SERVER.

7.  传递配置文件信息给RESTful API SERVER(第一次启动)

4.4 数据展示
------------

1.  对于已经存贮的数据，要能实时的读取；

2.  要保证查询的速度，保证在秒级或者毫秒级能查询到想要查询的结果；

3.  能够对关心的数据做分析；

4.  多元化展示，图形，图表等；

5.  能够对分析的数据做进步一步展示包括报表展示等。

6.  对于数据展现要有权限关系，防止非法访问，保护相关数据不被不相关人员获取

4.5 访问接口
------------

对于采集的数据系统要提供统一的数据访问接口。

1.  保证访问数据高速，有效，规整。

2. 可以通过不同的应用语言，不同的方式来使用接口。

3. 保证访问接口的数据安全，没有非法访问。

    在用户，提供相应的输入数据：例如访问主题、证书、数据id等，返回用户关心的数据。

4.6 数据存储
------------

对于采集到的数据在存储层，保证数据的一致性，没有重复；

要保证数据的安全，不容易丢失；

要有容错，当发生意外能保证数据恢复；

# 5. 数据结构

对于采集的数据要保证有统一的数据格式，以保证能更好的展示数据

基本要求包括：

1.  事件发生的日期和时间

2.  事件主体

3.  事件客体

4.  事件描述

5.  事件类型

6.  事件级别

7.  日志数据源的 IP 地址或名称

8.  参考《数据字段说明》

辅助信息包括：

1.  采集时间

2. 采集地址

3. 采集方式等

4. 设计实现
   =========

参考 《数据采集器设计文档》
